{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avrland/dataScienceFromScratch/blob/main/Niusbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "# #Niusbot\n",
        "Generating funny news titles, learned from bas of 20k news titles, gathered from polish news websites.\n",
        "\n",
        "Based on textgenrnn:\n",
        "https://minimaxir.com/2018/05/text-neural-networks/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJnnr7NkfH6y",
        "outputId": "0ea6460d-7f9b-489c-ac86-5cbc24549f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Montowanie Google Drive\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IqP-vcvZYtm",
        "outputId": "e710477a-c501-4276-8fb1-5f26939e3b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/avrland/textgenrnn.git\n",
            "  Cloning https://github.com/avrland/textgenrnn.git to /tmp/pip-req-build-y903byib\n",
            "  Running command git clone -q https://github.com/avrland/textgenrnn.git /tmp/pip-req-build-y903byib\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from textgenrnn==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from textgenrnn==2.0.0) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from textgenrnn==2.0.0) (4.63.0)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from textgenrnn==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (13.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.21.5)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.44.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (0.24.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->textgenrnn==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->textgenrnn==2.0.0) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.1.0->textgenrnn==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->textgenrnn==2.0.0) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->textgenrnn==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->textgenrnn==2.0.0) (1.1.0)\n",
            "Building wheels for collected packages: textgenrnn\n",
            "  Building wheel for textgenrnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textgenrnn: filename=textgenrnn-2.0.0-py3-none-any.whl size=1734430 sha256=ee60c6453bfbd380125139706dc561444dcb669e62fa41b49886ea293ddd96de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uy70lf8z/wheels/6c/a8/d5/514646cd30812010e24061dd9370a1b07e64bb6f2636b88943\n",
            "Successfully built textgenrnn\n",
            "Installing collected packages: tf-estimator-nightly, textgenrnn\n",
            "Successfully installed textgenrnn-2.0.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#!pip3 install tensorflow==2.3.2\n",
        "#!pip install -q textgenrnn\n",
        "!pip3 install git+https://github.com/avrland/textgenrnn.git\n",
        "from textgenrnn import textgenrnn\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTHu97pFcg5o"
      },
      "source": [
        "# Fetching title base\n",
        "We download base from github there and read first lines, to make sure everything was downloaded properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24iFkvjTccNN",
        "outputId": "e6177e7e-740a-4a3e-84fa-c61fa0b22ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=d91152291e50b1ae1261c7924b8a1109540e62f893e409f8b0322fb5260633b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Line 0: \n",
            "Line 1: Polka straciła 36 tys. zł: napastnik wykiwał zarówno ją, jak i bank\n",
            "Line 2: Chrome 86 na Androida pozwoli zaplanować pobieranie. Można już testować\n",
            "Line 3: Poczta Polska i cyfrowa rewolucja. Identyfikacja RFID przyspieszy wysyłki\n",
            "Line 4: GOG GALAXY 2.0 łączy siły z Epic Games Store. Jest wreszcie oficjalna integracja\n",
            "Database size: 76585\n"
          ]
        }
      ],
      "source": [
        "def lineCounter(filename):\n",
        "  file = open(filename, \"r\")\n",
        "  line_count = 0\n",
        "  for line in file:\n",
        "      if line != \"\\n\":\n",
        "          line_count += 1\n",
        "  return str(line_count)\n",
        "\n",
        "!pip install wget\n",
        "import wget\n",
        "url = 'https://raw.githubusercontent.com/avrland/polishNewsTitleDatabase/main/titles.txt'\n",
        "filename = '/content/titles.txt'  \n",
        "wget.download(url, filename)\n",
        "with open(filename) as fn:  \n",
        "  ln = fn.readline()\n",
        "  lncnt = 0\n",
        "  while lncnt < 5:\n",
        "       print(\"Line {}: {}\".format(lncnt, ln.strip()))\n",
        "       ln = fn.readline()\n",
        "       lncnt += 1\n",
        "print(\"Database size: \" + lineCounter(filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "# Model and training parameters\n",
        "Set the textgenrnn model configuration here: the default parameters here give good results for most workflows. (see the [demo notebook](https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb) for more information about these parameters)\n",
        "\n",
        "If you are using an input file where documents are line-delimited, make sure to set `line_delimited` to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8wSlgXoDPCR"
      },
      "outputs": [],
      "source": [
        "model_cfg = {\n",
        "    'word_level': True,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n",
        "    'rnn_size': 512,   # number of LSTM cells of each layer (128/256 recommended)\n",
        "    'rnn_layers': 3,   # number of LSTM layers (>=2 recommended)\n",
        "    'rnn_bidirectional': True,   # consider text both forwards and backward, can give a training boost\n",
        "    'max_length': 3,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n",
        "    'max_words': 10000,   # maximum number of words to model; the rest will be ignored (word-level model only)\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': True,   # set to True if each text has its own line in the source file\n",
        "    'num_epochs': 10,   # set higher to train the model for longer\n",
        "    'gen_epochs': 2,   # generates sample text from model after given number of epochs\n",
        "    'train_size': 0.9,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n",
        "    'dropout': 0.2,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n",
        "    'validation': True,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n",
        "    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "The next cell will start the actual training. And thanks to the power of Keras's CuDNN layers, training is super-fast when compared to CPU training on a local machine!\n",
        "\n",
        "Ideally, you want a training loss less than `1.0` in order for the model to create sensible text consistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49df03d3-7249-4108-cd9b-28aad735093e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76,585 texts collected.\n",
            "Training new model w/ 3-layer, 512-cell Bidirectional LSTMs\n",
            "Training on 970,981 word sequences.\n",
            "Epoch 1/10\n",
            "948/948 [==============================] - 311s 317ms/step - loss: 5.9102 - val_loss: 5.1767 - lr: 0.0040\n",
            "Epoch 2/10\n",
            "948/948 [==============================] - ETA: 0s - loss: 4.8524####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "koronawirus w polsce. najnowsze dane z sanepidu\n",
            "\n",
            "koronawirus w polsce. najnowsze dane z sanepidu\n",
            "\n",
            "koronawirus. ile nowych zakażeń w powiecie. zobacz dane na 19.. 2021\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "koronawirus. ile nowych zakażeń w powiecie. zobacz dane na 19.. 2021\n",
            "\n",
            "koronawirus. ile nowych zakażeń w powiecie - 9.. 2021 w powiecie. zobacz dane na 26.. 2021\n",
            "\n",
            "koronawirus. ile nowych zakażeń w powiecie. zobacz dane na 19\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "korea. leo messiego...\n",
            "\n",
            "koronawirus. w łodzi do restauracji. trzaskowski jeszcze oferują zgonów w tym tempie podczas budowy polskiego serialu podróży - piłkarski świat.\n",
            "\n",
            "tajemnice polskiego...\n",
            "\n",
            "948/948 [==============================] - 308s 325ms/step - loss: 4.8524 - val_loss: 4.9877 - lr: 0.0036\n",
            "Epoch 3/10\n",
            "948/948 [==============================] - 304s 321ms/step - loss: 4.5489 - val_loss: 4.9181 - lr: 0.0032\n",
            "Epoch 4/10\n",
            "948/948 [==============================] - ETA: 0s - loss: 4.2348####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci -. farm\n",
            "\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci -. farm\n",
            "\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci -. farm\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci - 29 kwietnia\n",
            "\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci - 12 kwietnia\n",
            "\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci - 22 kwietnia. ile osób zmarło ile zachorowań\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "vivo 3 - 7 i wideo\n",
            "\n",
            "kary za brak maseczki apple szykuje się broni piłkarski świat.\n",
            "\n",
            "profesor twórca wraca do żywych ludzi...\n",
            "\n",
            "948/948 [==============================] - 312s 329ms/step - loss: 4.2348 - val_loss: 5.0414 - lr: 0.0028\n",
            "Epoch 5/10\n",
            "948/948 [==============================] - 305s 321ms/step - loss: 4.0282 - val_loss: 5.0551 - lr: 0.0024\n",
            "Epoch 6/10\n",
            "948/948 [==============================] - ETA: 0s - loss: 3.8148"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "file_name = '/content/titles.txt'\n",
        "\n",
        "#Get current data and time for filename\n",
        "from datetime import datetime\n",
        "timeNow = str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "model_name = 'niusbot'  # change to set file name of resulting trained models/texts\n",
        "\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file_name,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=1024,\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=100,\n",
        "    word_level=model_cfg['word_level'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating titles from cached moded"
      ],
      "metadata": {
        "id": "zvw5_clv_-T6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMuDJoUPN7JX",
        "outputId": "8f39d8b1-c626-4f40-9f20-1ab794aed398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:01<00:10,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bardzo drogie złoto katastrofa. aż 650 testów na koronawirusa. przeciwciała utrzymują się do 13 miesięcy po zakażeniu\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:01<00:06,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w toruniu w polsce. zakażenia sars - - 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:02<00:05,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wicemarszałek senatu głosowanie sprzed to świetny nowy mecz 2 - krótka recenzja play\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:04<00:06,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chińczycy pokazali najszybszy pociąg w dom zamiast smartfonów – premiera. o polsce trafi do śmierci w kwietniu ze swoim dla wielu\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:04<00:04,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "krzysztof dostał wersję o zdjęcia\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:04<00:02,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hamilton opuszcza. policja znalazła sposób od kwietnia\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:06<00:02,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rosyjski dziennikarz jak król ale są zagrożeniem targi po roku. siatkówka polska - bułgaria 20 nowości. test\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:06<00:01,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oświaty 1 mld zł – inwestycje.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:07<00:00,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "andrzej wchodzi w świat biznesu służy\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:09<00:00,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rada odrzuciła wolne szczepionki przeciwko covid - 19 czy konkurencja dla google street view zdjęcia. co wiemy o produkcji nowego świata. na street. kiedy jest mecz polski - sport\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "textgen.generate(10, temperature=1.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving current model to Google Drive"
      ],
      "metadata": {
        "id": "ESZpFeAh_qw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(model_name + '.hdf5', '/content/drive/MyDrive/')\n",
        "shutil.copy(model_name + '.json', '/content/drive/MyDrive/')\n",
        "shutil.copy(model_name + '.json', '/content/drive/MyDrive/')"
      ],
      "metadata": {
        "id": "I_HjOyWs_thB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0tTx6NyOU6L"
      },
      "source": [
        "# Retraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P8nYG6iORuX",
        "outputId": "dfd2dfba-8e94-4c7b-86f4-fa74d4a25608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76,585 texts collected.\n",
            "Training on 971,095 word sequences.\n",
            "Epoch 1/4\n",
            "  6/948 [..............................] - ETA: 5:25 - loss: 1.4104WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1490s vs `on_train_batch_end` time: 0.1878s). Check your callbacks.\n",
            "948/948 [==============================] - 377s 397ms/step - loss: 2.3251 - val_loss: 2.7831 - lr: 0.0040\n",
            "Epoch 2/4\n",
            "948/948 [==============================] - ETA: 0s - loss: 1.8193####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "koronawirus w polsce. najnowsze dane o zakażeniach 15.. 2021\n",
            "\n",
            "koronawirus w polsce. najnowsze statystyki na 21.. 2021 dla wybranych krajów spoza europy. liczba zgonów i potwierdzo\n",
            "\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci - 9 maja\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci - 10 czerwca\n",
            "\n",
            "w galerii. jest bardziej\n",
            "\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci - 19 czerwca\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "od poniedziałku rusza program pilotażowy\n",
            "\n",
            "zakażonych koronawirusem. rodzice w 3 infekcje. uczniowie wracają do szkół. dariusz piontkowski w se będzie mieć\n",
            "\n",
            "w ubiegłym roku ponad 10 mln mieszkańców z powodu covid - 19 - najnowsze wyniki badań\n",
            "\n",
            "948/948 [==============================] - 399s 420ms/step - loss: 1.8193 - val_loss: 2.7215 - lr: 0.0030\n",
            "Epoch 3/4\n",
            "948/948 [==============================] - 394s 416ms/step - loss: 1.4919 - val_loss: 2.6751 - lr: 0.0020\n",
            "Epoch 4/4\n",
            "948/948 [==============================] - ETA: 0s - loss: 1.2874####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "koronawirus. w polsce zmarły 3 osoby w lubuskiem 1 nowe zakażenie\n",
            "\n",
            "koronawirus. w polsce zmarły 3 osoby w lubuskiem 1 nowe zakażenie\n",
            "\n",
            "koronawirus w polsce. nowe przypadki zakażenia i śmierci - 15 maja\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "koronawirus. w podlaskiem 5 nowych przypadków. zmarło 9 osób\n",
            "\n",
            "nie tylko koronawirus. naukowcy alarmują kolejny groźny wirus\n",
            "\n",
            "koronawirus w polsce. nowe przypadki i ofiary śmiertelne. mz podaje dane 9 czerwca\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "z gwiezdnych mógł być ze swoich\n",
            "\n",
            "koalicja obywatelska spotka się w sprawie wyboru nowego rzecznika praw obywatelskich\n",
            "\n",
            "sondaż dla rmf fm i adama dworczyk o połowę\n",
            "\n",
            "948/948 [==============================] - 398s 420ms/step - loss: 1.2874 - val_loss: 2.7176 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "file_name = '/content/titles.txt'\n",
        "\n",
        "#Pobierz dzisiejszą datę i godzinę\n",
        "from datetime import datetime\n",
        "timeNow = str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "model_name = 'niusbot_' + timeNow   # change to set file name of resulting trained models/texts\n",
        "\n",
        "#textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file_name,\n",
        "    new_model=False,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=1024,\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=100,\n",
        "    word_level=model_cfg['word_level'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Zjtsb_Dgj-"
      },
      "source": [
        "# Generating titles from saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTKiTogyrm9h",
        "outputId": "6cff5067-7230-41cd-a6c3-0d88b41ab877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "modelPath = \"/content/drive/MyDrive/NIUSBOT/titles\"\n",
        "textgen = textgenrnn(weights_path=modelPath + '_weights.hdf5',\n",
        "                       vocab_path=modelPath + '_vocab.json',\n",
        "                       config_path=modelPath + '_config.json')\n",
        "textgen.generate(10, temperature=1.1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Niusbot",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}